{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKsFUPdDnA6G"
      },
      "source": [
        "# Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Может возникать ошибка с загрузкой бибилиотеки eli5 поэтому её функционал закомментирован."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U spacy\n",
        "# !pip install eli5\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QlERLh6-_gpM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import spacy\n",
        "import warnings\n",
        "# import eli5\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d uciml/sms-spam-collection-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ZIP_PATH = input(\"\\nУкажите полный путь к .zip файлу:\\n\")\n",
        "\n",
        "with ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "USERNAME = input(\"Укажите ваш ник в Kaggle:\\n\")\n",
        "KEY = input(\"\\nУкажите ваш токен в Kaggle:\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5DBN5XXzktC"
      },
      "outputs": [],
      "source": [
        "api_token = {\"username\": USERNAME, \"key\": KEY}\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w+') as file:\n",
        "    json.dump(api_token, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5RX4ur65zkT_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSjL2MOmydAc"
      },
      "source": [
        "# Набор данных\n",
        "[\n",
        "SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)\n",
        "\n",
        "SMS Spam Collection - это множество СМС сообщений, которые были получены согласно исследованию \"SMS Spam\". Оно содержит одно множество из 5574 английских СМС сообщений, отмеченные как **ham** (не спам) или **spam** (спам)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oBMYgpJW5E9I"
      },
      "outputs": [],
      "source": [
        "FILE_PATH = input(\"\\nУкажите полный путь к .csv файлу:\\n\")\n",
        "data = pd.read_csv(FILE_PATH, encoding='iso-8859-1')[['v1', 'v2']].rename(columns={'v1': 'label', 'v2': 'text'})\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEHyPWhH3y5A"
      },
      "source": [
        "# Очистка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jbJXNs0x0ugi"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "data['cleaned_text'] = data['text'].apply(\n",
        "    lambda x: ' '.join(\n",
        "        token.lemma_.lower() for token in nlp(x) if\n",
        "        not token.is_stop\n",
        "        and not token.is_punct\n",
        "        and not token.is_digit\n",
        "        and token.is_ascii\n",
        "        and not token.like_email\n",
        "        and not token.like_num\n",
        "        and not token.like_url\n",
        "        and not token.is_space\n",
        "    )\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['label'], test_size=0.1, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJaVRwtdSd2T"
      },
      "source": [
        "# TfidfTransformer vs  TfidfVectorizer vs CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv6Xv9hhBkYQ"
      },
      "outputs": [],
      "source": [
        "def custom_tokenize(text):\n",
        "  text = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "  return text.split()\n",
        "\n",
        "# CountVectorizer\n",
        "custom_vectorizer = CountVectorizer(\n",
        "    max_df=0.7,\n",
        "    min_df=0.003,\n",
        "    tokenizer=custom_tokenize,\n",
        ")\n",
        "pipe = Pipeline(\n",
        "    steps=[\n",
        "        ('counter', CountVectorizer()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ]\n",
        ").fit(X_train, y_train)\n",
        "preds = pipe.predict(X_test)\n",
        "print(f\"CountVectorizer:\\n{classification_report(y_test, preds)}\\n\")\n",
        "\n",
        "# TfidfTransformer\n",
        "pipe = Pipeline(\n",
        "    steps=[\n",
        "        ('counter', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ]\n",
        ").fit(X_train, y_train)\n",
        "preds = pipe.predict(X_test)\n",
        "print(f\"TfidfTransformer:\\n{classification_report(y_test, preds)}\\n\")\n",
        "\n",
        "# TfidfVectorizer\n",
        "pipe = Pipeline(\n",
        "    steps=[\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ]\n",
        ").fit(X_train, y_train)\n",
        "preds = pipe.predict(X_test)\n",
        "print(f\"TfidfVectorizer:\\n{classification_report(y_test, preds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9j6fJk4kTLc"
      },
      "source": [
        "Судя по результатам, CountVectorizer лучше подходит в качестве векторизации для данного датасета (проверено не только для логистической регрессии), т.к. имеет лучшие f1-меру, TP, TN, FP, FN, взвешенное среднее и макросреднее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68KVgikQBxbR"
      },
      "source": [
        "# Логистическая регрессия"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tqzvj2rnoT4_"
      },
      "outputs": [],
      "source": [
        "def plot_grid_search(grid_search):\n",
        "  '''\n",
        "  Отрисовка графика подбора гиперпараметров\n",
        "  '''\n",
        "  results = pd.DataFrame(grid_search.cv_results_)\n",
        "  results[\"params_str\"] = results.params.apply(str)\n",
        "  results.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\n",
        "  mean_scores = results.pivot(\n",
        "      index=\"iter\",\n",
        "      columns=\"params_str\",\n",
        "      values=\"mean_test_score\"\n",
        "  )\n",
        "  ax = mean_scores.plot(legend=False, alpha=0.6)\n",
        "  labels = [\n",
        "      f\"iter={i}\\nn_samples={grid_search.n_resources_[i]}\\nn_candidates={grid_search.n_candidates_[i]}\"\n",
        "      for i in range(grid_search.n_iterations_)\n",
        "  ]\n",
        "  ax.set_xticks(range(grid_search.n_iterations_))\n",
        "  ax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\n",
        "  ax.set_title(\"Scores of candidates over iterations\")\n",
        "  ax.set_ylabel(\"Mean test score\", fontsize=15)\n",
        "  ax.set_xlabel(\"Iterations\", fontsize=15)\n",
        "  plt.tight_layout()\n",
        "  plt.grid()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJtnItit8lRA"
      },
      "outputs": [],
      "source": [
        "pipe_log = Pipeline(\n",
        "    steps=[\n",
        "        ('counter', CountVectorizer()),\n",
        "        ('clf', LogisticRegression())\n",
        "    ]\n",
        ")\n",
        "\n",
        "parameter_grid = {\n",
        "    \"counter__max_df\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
        "    \"counter__min_df\": [0.001, 0.003, 0.005],\n",
        "    \"counter__ngram_range\": ((1, 1), (1, 2)),\n",
        "    \"clf__solver\": [\"lbfgs\", \"liblinear\"],\n",
        "    \"clf__C\": np.linspace(0.1, 1, 10)\n",
        "}\n",
        "\n",
        "grid_search = HalvingGridSearchCV(\n",
        "    pipe_log,\n",
        "    param_grid=parameter_grid,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    cv=2,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        ").fit(X_train, y_train)\n",
        "print(f\"\\nЛучшие параметры для логистической регрессии: {grid_search.best_params_}\\n\")\n",
        "\n",
        "# Отрисовка графика\n",
        "plot_grid_search(grid_search)\n",
        "\n",
        "print(f\"\\nЛучший результат для логистической регрессии: {grid_search.best_score_}\\n\")\n",
        "\n",
        "# eli5.show_weights(\n",
        "#     estimator=grid_search.best_estimator_['clf'],\n",
        "#     feature_names=list(grid_search.best_estimator_['counter'].get_feature_names_out()),\n",
        "#     top=(20, 5)\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHITQgNoBy4c"
      },
      "source": [
        "# Решающие деревья"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4M60vJzRt9Q"
      },
      "outputs": [],
      "source": [
        "pipe_tree = Pipeline(\n",
        "    steps=[\n",
        "        ('counter', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', DecisionTreeClassifier())\n",
        "    ]\n",
        ")\n",
        "\n",
        "parameter_grid_tree = {\n",
        "    \"counter__max_df\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
        "    \"counter__min_df\": [0.001, 0.003, 0.005],\n",
        "    \"counter__ngram_range\": ((1, 1), (1, 2)),\n",
        "    \"tfidf__norm\": (\"l1\", \"l2\"),\n",
        "    \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
        "    \"clf__splitter\": [\"best\", \"random\"],\n",
        "    \"clf__max_depth\": range(1, 6),\n",
        "    \"clf__min_samples_split\": [2, 3]\n",
        "}\n",
        "\n",
        "grid_search_tree = HalvingGridSearchCV(\n",
        "    pipe_tree,\n",
        "    param_grid=parameter_grid_tree,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    cv=2,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        ").fit(X_train, y_train)\n",
        "print(f\"\\nЛучшие параметры для решаюших деревьев: {grid_search_tree.best_params_}\\n\")\n",
        "\n",
        "# Отрисовка графика\n",
        "plot_grid_search(grid_search_tree)\n",
        "\n",
        "print(f\"\\nЛучший результат для решаюших деревьев: {grid_search_tree.best_score_}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBoQ1sagkFHS"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciRNi0iEbAet"
      },
      "outputs": [],
      "source": [
        "pipe_NB = Pipeline(\n",
        "    steps=[\n",
        "        ('counter', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', ComplementNB())\n",
        "    ]\n",
        ")\n",
        "\n",
        "parameter_grid_NB = {\n",
        "    \"counter__max_df\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
        "    \"counter__min_df\": [0.001, 0.003, 0.005],\n",
        "    \"counter__ngram_range\": ((1, 1), (1, 2)),\n",
        "    \"tfidf__norm\": (\"l1\", \"l2\"),\n",
        "    \"clf__alpha\": [0, True],\n",
        "    \"clf__norm\": [True, False]\n",
        "}\n",
        "\n",
        "grid_search_NB = HalvingGridSearchCV(\n",
        "    pipe_NB,\n",
        "    param_grid=parameter_grid_NB,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    cv=2,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        ").fit(X_train, y_train)\n",
        "print(f\"\\nЛучшие параметры для Наивного Байеса: {grid_search_NB.best_params_}\\n\")\n",
        "\n",
        "# Отрисовка графика\n",
        "plot_grid_search(grid_search_NB)\n",
        "\n",
        "print(f\"\\nЛучший результат для Наивного Байеса: {grid_search_NB.best_score_}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG-smX6AcvmL"
      },
      "outputs": [],
      "source": [
        "# Сравнение на отложенной выборке\n",
        "log_preds = grid_search.best_estimator_.predict(X_test)\n",
        "print(f\"Лучший результат для логистической регрессии:\\n{classification_report(y_test, log_preds)}\")\n",
        "tree_preds = grid_search_tree.best_estimator_.predict(X_test)\n",
        "print(f\"\\nЛучший результат для решаюших деревьев:\\n{classification_report(y_test, tree_preds)}\")\n",
        "nb_preds = grid_search_NB.best_estimator_.predict(X_test)\n",
        "print(f\"\\nЛучший результат для Наивного Байеса:\\n{classification_report(y_test, nb_preds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDoMfNeJjMuO"
      },
      "source": [
        "По всем метрикам классификатор на деревьях и Наивный Байес значительно уступают логистической регрессии."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
